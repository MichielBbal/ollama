{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basics of RAG from scratch\n",
    "\n",
    "With this notebook you can ask questions about your own document. \n",
    "It uses ollama to run LLM's locally. \n",
    "\n",
    "Make sure you have downloaded and installed ollama from www.ollama.com.\n",
    "\n",
    "#### Contents\n",
    "0. Install and import packages\n",
    "1. Check available models in ollama + set system prompt\n",
    "2. Get the text document and split into paragraphs (chunks of text)\n",
    "3. EmbeddingsÂ¶\n",
    "4. Set the prompt, create prompt embeddings and do similarity search\n",
    "5. Get response from the LLM\n",
    "\n",
    "#### Source\n",
    "source & acknowledgements: https://decoder.sh/videos/rag-from-the-ground-up-with-python-and-ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the flow in this flowdiagram. Please study it carefully.\n",
    "![slide1](slide_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.5.0)\n",
      "Requirement already satisfied: certifi in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2.10)\n",
      "Requirement already satisfied: sniffio in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/michielbontenbal/anaconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ollama\n",
    "%pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import ollama\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check availabe models in ollama + set system prompt\n",
    "\n",
    "For this notebook you'll need two models:\n",
    "- an embedding model: nomic-embed-text\n",
    "- any LLM: tinyllama, mistral or other (choose yourself, set later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                            \tID          \tSIZE  \tMODIFIED     \n",
      "bramvanroy/fietje-2b-chat:Q3_K_M\t6dd6525c1e6c\t1.4 GB\t2 weeks ago \t\n",
      "llava:latest                    \t8dd30f6b0cb1\t4.7 GB\t10 hours ago\t\n",
      "mistral:latest                  \t61e88e884507\t4.1 GB\t2 weeks ago \t\n",
      "nomic-embed-text:latest         \t0a109f422b47\t274 MB\t2 weeks ago \t\n",
      "tinyllama:latest                \t2644915ede35\t637 MB\t4 months ago\t\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#uncomment if necessary\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#set the system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful reading assistant who answers questions \n",
    "        based on snippets of text provided in context. Answer only using the context provided, \n",
    "        being as concise as possible. If you're unsure, just say that you don't know.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the text document and split into paragraphs (chunks of text)\n",
    "\n",
    "In this case, we simply use .txt files from the Project Gutenberg website.\n",
    "www.gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# function to open a file and return paragraphs\n",
    "def parse_file(filename):\n",
    "    with open(filename, encoding=\"utf-8-sig\") as f:\n",
    "        paragraphs = []\n",
    "        buffer = []\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                buffer.append(line)\n",
    "            elif len(buffer):\n",
    "                paragraphs.append((\" \").join(buffer))\n",
    "                buffer = []\n",
    "        if len(buffer):\n",
    "            paragraphs.append((\" \").join(buffer))\n",
    "        return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Project Gutenberg eBook of Peter Pan', 'This ebook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this ebook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.', 'Title: Peter Pan']\n",
      "Total number of paragraphs: 1736\n"
     ]
    }
   ],
   "source": [
    "# open file as provided in the GitHub repo\n",
    "filename = \"peter-pan.txt\"\n",
    "paragraphs = parse_file(filename)\n",
    "print(paragraphs[0:3]) #print first 3 paragraphs\n",
    "print(f'Total number of paragraphs: {len(paragraphs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# functions to save, load and get the embeddings\n",
    "def save_embeddings(filename, embeddings):\n",
    "    # create dir if it doesn't exist\n",
    "    if not os.path.exists(\"embeddings\"):\n",
    "        os.makedirs(\"embeddings\")\n",
    "    # dump embeddings to json\n",
    "    with open(f\"embeddings/{filename}.json\", \"w\") as f:\n",
    "        json.dump(embeddings, f)\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    # check if file exists\n",
    "    if not os.path.exists(f\"embeddings/{filename}.json\"):\n",
    "        return False\n",
    "    # load embeddings from json\n",
    "    with open(f\"embeddings/{filename}.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_embeddings(filename, modelname, chunks):\n",
    "    # check if embeddings are already saved\n",
    "    if (embeddings := load_embeddings(filename)) is not False:\n",
    "        return embeddings\n",
    "    # get embeddings from ollama\n",
    "    embeddings = [\n",
    "        ollama.embeddings(model=modelname, prompt=chunk)[\"embedding\"]\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    # save embeddings\n",
    "    save_embeddings(filename, embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1736"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the embeddings\n",
    "embeddings = get_embeddings(filename, \"nomic-embed-text\", paragraphs)\n",
    "len(embeddings) #should be same number as paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "## 4. Set the prompt, create prompt embeddings, do similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#set the prompt\n",
    "prompt = \"Tell me about tinke bell?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create the prompt embeddings. Use the same embedding model!\n",
    "prompt_embedding = ollama.embeddings(model=\"nomic-embed-text\", prompt=prompt)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# find cosine similarity of every chunk to a given embedding\n",
    "def find_most_similar(needle, haystack):\n",
    "    needle_norm = norm(needle)\n",
    "    similarity_scores = [\n",
    "        np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack\n",
    "    ]\n",
    "    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6463232225153855, 1576),\n",
       " (0.6360647070442861, 252),\n",
       " (0.6153480329856226, 237),\n",
       " (0.6082050651637667, 1024),\n",
       " (0.6079815685485532, 176)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find paragraphs most similar to the prompt\n",
    "most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:5]\n",
    "most_similar_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get a response from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#set your model here!\n",
    "model='mistral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " Tinker Bell is a fairy who mends pots and kettles. She can be found in various places and has been keeping quiet. She used the phrase \"you silly ass\" when expressing frustration.\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(\n",
    "        model= model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT\n",
    "                + \"\\n\".join(paragraphs[item[1]] for item in most_similar_chunks),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "print(\"\\n\\n\")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
