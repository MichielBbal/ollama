{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basics of RAG from scratch\n",
    "\n",
    "With this notebook you can ask questions about your own document. \n",
    "It uses ollama to run LLM's locally. \n",
    "\n",
    "Make sure you have downloaded and installed ollama from www.ollama.com.\n",
    "\n",
    "#### Contents\n",
    "0. Install and import packages\n",
    "1. Check available models in ollama + set system prompt\n",
    "2. Get the text document and split into paragraphs (chunks of text)\n",
    "3. EmbeddingsÂ¶\n",
    "4. Set the prompt, create prompt embeddings and do similarity search\n",
    "5. Get response from the LLM\n",
    "\n",
    "#### Source\n",
    "source & acknowledgements: https://decoder.sh/videos/rag-from-the-ground-up-with-python-and-ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the flow in this flowdiagram. Please study it carefully.\n",
    "![slide1](slide_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ollama\n",
    "%pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import ollama\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check availabe models in ollama + set system prompt\n",
    "\n",
    "For this notebook you'll need two models:\n",
    "- an embedding model: nomic-embed-text\n",
    "- any LLM: tinyllama, mistral or other (choose yourself, set later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment if necessary\n",
    "#!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#set the system prompt\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful reading assistant who answers questions \n",
    "        based on snippets of text provided in context. Answer only using the context provided, \n",
    "        being as concise as possible. If you're unsure, just say that you don't know.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get the text document and split into paragraphs (chunks of text)\n",
    "\n",
    "In this case, we simply use .txt files from the Project Gutenberg website.\n",
    "www.gutenberg.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# function to open a file and return paragraphs\n",
    "def parse_file(filename):\n",
    "    with open(filename, encoding=\"utf-8-sig\") as f:\n",
    "        paragraphs = []\n",
    "        buffer = []\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                buffer.append(line)\n",
    "            elif len(buffer):\n",
    "                paragraphs.append((\" \").join(buffer))\n",
    "                buffer = []\n",
    "        if len(buffer):\n",
    "            paragraphs.append((\" \").join(buffer))\n",
    "        return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# open file as provided in the GitHub repo\n",
    "filename = \"peter-pan.txt\"\n",
    "paragraphs = parse_file(filename)\n",
    "print(paragraphs[0:3]) #print first 3 paragraphs\n",
    "print(f'Total number of paragraphs: {len(paragraphs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# functions to save, load and get the embeddings\n",
    "def save_embeddings(filename, embeddings):\n",
    "    # create dir if it doesn't exist\n",
    "    if not os.path.exists(\"embeddings\"):\n",
    "        os.makedirs(\"embeddings\")\n",
    "    # dump embeddings to json\n",
    "    with open(f\"embeddings/{filename}.json\", \"w\") as f:\n",
    "        json.dump(embeddings, f)\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    # check if file exists\n",
    "    if not os.path.exists(f\"embeddings/{filename}.json\"):\n",
    "        return False\n",
    "    # load embeddings from json\n",
    "    with open(f\"embeddings/{filename}.json\", \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_embeddings(filename, modelname, chunks):\n",
    "    # check if embeddings are already saved\n",
    "    if (embeddings := load_embeddings(filename)) is not False:\n",
    "        return embeddings\n",
    "    # get embeddings from ollama\n",
    "    embeddings = [\n",
    "        ollama.embeddings(model=modelname, prompt=chunk)[\"embedding\"]\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    # save embeddings\n",
    "    save_embeddings(filename, embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#get the embeddings\n",
    "embeddings = get_embeddings(filename, \"nomic-embed-text\", paragraphs)\n",
    "len(embeddings) #should be same number as paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "metadata": {}
   },
   "source": [
    "## 4. Set the prompt, create prompt embeddings, do similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#set the prompt\n",
    "prompt = \"Tell me about tinke bell?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create the prompt embeddings. Use the same embedding model!\n",
    "prompt_embedding = ollama.embeddings(model=\"nomic-embed-text\", prompt=prompt)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# find cosine similarity of every chunk to a given embedding\n",
    "def find_most_similar(needle, haystack):\n",
    "    needle_norm = norm(needle)\n",
    "    similarity_scores = [\n",
    "        np.dot(needle, item) / (needle_norm * norm(item)) for item in haystack\n",
    "    ]\n",
    "    return sorted(zip(similarity_scores, range(len(haystack))), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# find paragraphs most similar to the prompt\n",
    "most_similar_chunks = find_most_similar(prompt_embedding, embeddings)[:5]\n",
    "most_similar_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get a response from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set your model here!\n",
    "model='mistral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "response = ollama.chat(\n",
    "        model= model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT\n",
    "                + \"\\n\".join(paragraphs[item[1]] for item in most_similar_chunks),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "print(\"\\n\\n\")\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
