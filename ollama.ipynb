{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c409dcaf",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "Ollama is a tool that allows users to run open-source large language models (LLMs) locally on your laptop. Ollama supports a variety of models, including Llama2, Mistral, CodeLlama and many others. \n",
    "\n",
    "You'll need to download ollama first. Download it from www.ollama.com.\n",
    "\n",
    "Courtesy of some code examples to ollama.com / Jeffrey Morgan.\n",
    "License: MIT License\n",
    "\n",
    "### Contents\n",
    "0. Install and settings\n",
    "1. First script\n",
    "2. Run with streaming data\n",
    "3. Create a gradio front end\n",
    "\n",
    "### Sources\n",
    "- https://github.com/ollama/ollama-python\n",
    "- https://github.com/ollama/ollama/blob/main/docs/api.md#api\n",
    "- https://pypi.org/project/ollama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669c8d4",
   "metadata": {},
   "source": [
    "## 0. Check your version, install ollama python package\n",
    "\n",
    "*Before running this code, make sure you've installed ollama on your laptop!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbaed4",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Check your version of python. To run ollama with python you will need Python 3.8 or higher.\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd224ea",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215465c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "!ollama.pull('mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa657e4",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Script that shows the models on your laptop.\n",
    "import ollama\n",
    "models_dict = ollama.list()\n",
    "models = models_dict['models']\n",
    "model_list = []\n",
    "for i in range(len(models)):\n",
    "    print(models[i]['name'])\n",
    "    model_list.append(models[i]['name'])\n",
    "print(50*'-')\n",
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446fb78",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#printing the details of a model\n",
    "ollama.show('llava')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad15d86",
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting the size of each model\n",
    "for model in model_list:\n",
    "    model_info = ollama.show(model)\n",
    "    print(f\"{model}: {model_info['details']['parameter_size']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76635e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#show all functions\n",
    "print(dir(ollama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9609c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Delete a model\n",
    "#ollama.delete('starcoder:3B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1ae15",
   "metadata": {},
   "source": [
    "## 1. Run first script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ccf41",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#first script from ollama website (https://github.com/ollama/ollama-python)\n",
    "import ollama\n",
    "response = ollama.chat(model='tinyllama', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9af2af5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer to your question \"Why is the sky blue?\" is complex and multifaceted, but there are a few reasons why it appears blue in the sky:\n",
      "\n",
      "1. Blue light: The sun's visible light (wavelength between around 400 nm to about 700 nm) is mainly made up of two colors: red and blue. When the sun shines on Earth, it reflects blue light back into the atmosphere, which creates the color we see as \"blue.\"\n",
      "\n",
      "2. Clouds: The atmosphere above us is made up of a combination of gases (mostly nitrogen and oxygen) that absorb different colors of light. When sunlight hits these molecules, it causes the molecules to vibrate at a specific frequency and bounce back into the atmosphere, creating visible light.\n",
      "\n",
      "3. Temperature: The temperature of the upper layers of the Earth's atmosphere is about 14 kilometers (8.7 miles) above sea level, where air pressure is lower than at sea level. As you go higher up, the temperature decreases, and so does the amount of blue light that can be absorbed by the atmosphere.\n",
      "\n",
      "Overall, it's impossible to say exactly why the sky appears blue due to all these factors. However, it's a beautiful and fascinating sight for millions of people around the world who look up at the night sky every day!\n"
     ]
    }
   ],
   "source": [
    "#as above but now as a function\n",
    "import ollama\n",
    "\n",
    "def ask_ollama(question):\n",
    "    \"\"\"\n",
    "    \n",
    "    Sends a question to the Ollama API and returns the response.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model='tinyllama',\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': question},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "# Example usage\n",
    "response_content = ask_ollama(\"Why is the sky blue?\")\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e149d",
   "metadata": {},
   "source": [
    "## 2. Streaming the response\n",
    "\n",
    "With streaming the response will be printed on the screen while the LLM is still busy generating the answer. This is a faster solution. Try it out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377debd",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "question = input('Your question:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645df074",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#source: https://github.com/ollama/ollama-python\n",
    "import ollama\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='tinyllama',\n",
    "    messages=[{'role': 'user', 'content': question}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e898ef",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#same but now as a function (to use with gradio) \n",
    "import ollama\n",
    "\n",
    "def ollama_chat_stream(question):\n",
    "    \"\"\"\n",
    "    Streams the chat response from Ollama using the 'tinyllama' model.\n",
    "    \"\"\"\n",
    "    # Initialize the chat with Ollama\n",
    "    stream = ollama.chat(\n",
    "        model='tinyllama',\n",
    "        messages=[{'role': 'user', 'content': question}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    # Stream and print the responses\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "        #print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "# Example usage\n",
    "ollama_chat_stream(\"Why is the sky blue?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a70748",
   "metadata": {},
   "source": [
    "## 3. Creating a gradio front end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809df18",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pip install gradio\n",
    "def chatbot_function(user_input):\n",
    "    return f\"You said: {user_input}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a11538",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a Gradio frontend make sure you have run previous cells\n",
    "import gradio as gr\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=ask_ollama,  #use the function we defined in a previous cell\n",
    "    inputs=\"text\", \n",
    "    outputs= \"text\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb9adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c17c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
