{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c409dcaf",
   "metadata": {},
   "source": [
    "# Ollama\n",
    "\n",
    "Ollama is a tool that allows users to run open-source large language models (LLMs) locally on your laptop. Ollama supports a variety of models, including Llama2, Mistral, CodeLlama and many others. \n",
    "\n",
    "You'll need to download ollama first. Download it from www.ollama.com.\n",
    "\n",
    "Courtesy of some code examples to ollama.com / Jeffrey Morgan.\n",
    "License: MIT License\n",
    "\n",
    "### Contents\n",
    "0. Install and settings\n",
    "1. First script\n",
    "2. Run with streaming data\n",
    "3. Create a gradio front end\n",
    "\n",
    "### Sources\n",
    "- https://github.com/ollama/ollama-python\n",
    "- https://github.com/ollama/ollama/blob/main/docs/api.md#api\n",
    "- https://pypi.org/project/ollama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669c8d4",
   "metadata": {},
   "source": [
    "## 0. Check your version, install ollama python package\n",
    "\n",
    "*Before running this code, make sure you've installed ollama on your laptop!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbaed4",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Check your version of python. To run ollama with python you will need Python 3.8 or higher.\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd224ea",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215465c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "!ollama.pull('mistral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa657e4",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Script that shows the models on your laptop.\n",
    "import ollama\n",
    "models_dict = ollama.list()\n",
    "models = models_dict['models']\n",
    "model_list = []\n",
    "for i in range(len(models)):\n",
    "    print(models[i]['name'])\n",
    "    model_list.append(models[i]['name'])\n",
    "print(50*'-')\n",
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446fb78",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#printing the details of a model\n",
    "ollama.show('llava')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad15d86",
   "metadata": {
    "metadata": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Getting the size of each model\n",
    "for model in model_list:\n",
    "    model_info = ollama.show(model)\n",
    "    print(f\"{model}: {model_info['details']['parameter_size']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76635e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#show all functions\n",
    "print(dir(ollama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe9609c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Delete a model\n",
    "#ollama.delete('starcoder:3B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1ae15",
   "metadata": {},
   "source": [
    "## 1. Run first script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ccf41",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#first script from ollama website (https://github.com/ollama/ollama-python)\n",
    "import ollama\n",
    "response = ollama.chat(model='tinyllama', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af2af5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#as above but now as a function\n",
    "import ollama\n",
    "\n",
    "def ask_ollama(question):\n",
    "    \"\"\"\n",
    "    \n",
    "    Sends a question to the Ollama API and returns the response.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model='tinyllama',\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': question},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response['message']['content']\n",
    "\n",
    "# Example usage\n",
    "response_content = ask_ollama(\"Why is the sky blue?\")\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e149d",
   "metadata": {},
   "source": [
    "## 2. Streaming the response\n",
    "\n",
    "With streaming the response will be printed on the screen while the LLM is still busy generating the answer. This is a faster solution. Try it out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377debd",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "question = input('Your question:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645df074",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#source: https://github.com/ollama/ollama-python\n",
    "import ollama\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='tinyllama',\n",
    "    messages=[{'role': 'user', 'content': question}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e898ef",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#same but now as a function (to use with gradio) \n",
    "import ollama\n",
    "\n",
    "def ollama_chat_stream(question):\n",
    "    \"\"\"\n",
    "    Streams the chat response from Ollama using the 'tinyllama' model.\n",
    "    \"\"\"\n",
    "    # Initialize the chat with Ollama\n",
    "    stream = ollama.chat(\n",
    "        model='tinyllama',\n",
    "        messages=[{'role': 'user', 'content': question}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    # Stream and print the responses\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)\n",
    "        #print(chunk['message']['content'], end='', flush=True)\n",
    "\n",
    "# Example usage\n",
    "ollama_chat_stream(\"Why is the sky blue?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a70748",
   "metadata": {},
   "source": [
    "## 3. Creating a gradio front end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809df18",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pip install gradio\n",
    "def chatbot_function(user_input):\n",
    "    return f\"You said: {user_input}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a11538",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#a Gradio frontend make sure you have run previous cells\n",
    "import gradio as gr\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=ask_ollama,  #use the function we defined in a previous cell\n",
    "    inputs=\"text\", \n",
    "    outputs= \"text\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb9adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c17c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
