{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a53c74b",
   "metadata": {},
   "source": [
    "# Find similar images using CLIP embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6c51e",
   "metadata": {},
   "source": [
    "In this notebook you learn how to find similar images using the cosine similarity.\n",
    "\n",
    "First, we'll get an understanding how the maths work using a basic example\n",
    "Second, we'll calculatie the similarity between two images\n",
    "Third, we'll find similar images from a set of images. \n",
    "\n",
    "### TO DO: Run the notebook and do the exercises\n",
    "\n",
    "### Contents\n",
    "1. Intro:  calculate the cosine similarity (nl: cosinusgelijkenis)\n",
    "2. Calculate cosine similarity for 2 images\n",
    "3. Find similar images with cosine similarity\n",
    "\n",
    "### Sources\n",
    "- https://nl.wikipedia.org/wiki/Cosinusgelijkenis\n",
    "- https://openai.com/research/clip\n",
    "- https://medium.com/@jeremy-k/unlocking-openai-clip-part-2-image-similarity-bf0224ab5bb0\n",
    "- https://www.geeksforgeeks.org/how-to-calculate-cosine-similarity-in-python/\n",
    "- https://www.geeksforgeeks.org/python-measure-similarity-between-two-sentences-using-cosine-similarity/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0cdc3",
   "metadata": {},
   "source": [
    "## 1. Intro:  calculate the cosine similarity (nl: cosinusgelijkenis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28728357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define two points\n",
    "\n",
    "# Choose values [1-10] (integers/floats) to properly plot it below\n",
    "\n",
    "# point 1\n",
    "p1x = 4\n",
    "p1y = 3\n",
    "\n",
    "#point 2\n",
    "p2x = 3\n",
    "p2y = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e285af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = sum(x * y for x, y in zip(a, b))\n",
    "    magnitude_a = sum(x * x for x in a) ** 0.5\n",
    "    magnitude_b = sum(x * x for x in b) ** 0.5\n",
    "    return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "cosine_similarity((p1x,p1y), (p2x,p2y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4c942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the points and create a vector from 0,0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#get points and plot them\n",
    "points_x = p1x, p2x\n",
    "points_y = p1y, p2y\n",
    "plt.plot(points_x, points_y, 'o') \n",
    "\n",
    "#plot vectors \n",
    "plt.quiver([0, 0], [0, 0], [p1x, p2x], [p1y,p2y], angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "#plot axes and show it\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e469b",
   "metadata": {},
   "source": [
    "### Exercise 1: \n",
    "1. play with the points and see what happens\n",
    "2. create two examples:\n",
    "    - Where cosine similarity is 1\n",
    "    - Where cosine similarity is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb055cc",
   "metadata": {},
   "source": [
    "## 2. Calculate cosine similarity for 2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first install packages! Uncomment if necessary\n",
    "%pip install torch torchvision\n",
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fdd63",
   "metadata": {},
   "source": [
    "Here we will use two images and calculate how similar they are. \n",
    "\n",
    "To do so we'll create embeddings using CLIP. CLIP has several ways of creating embeddings, but here we'll use a standard Vision Transformer (ViT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "\n",
    "#check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#select embedding model. We use a Vision Transformer (ViT) embedding.\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, use the images given. This is a similar image of car (one is smaller)\n",
    "image1 = \"car512.png\"\n",
    "image2= \"car256.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b24c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embeddings for the images\n",
    "image1_preprocess = preprocess(Image.open(image1)).unsqueeze(0).to(device)\n",
    "image1_features = model.encode_image( image1_preprocess)\n",
    "\n",
    "image2_preprocess = preprocess(Image.open(image2)).unsqueeze(0).to(device)\n",
    "image2_features = model.encode_image( image2_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370082b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the cosine similarity\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "similarity = cos(image1_features[0],image2_features[0]).item()\n",
    "similarity = (similarity+1)/2\n",
    "\n",
    "print(\"Image similarity: \", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b9f76",
   "metadata": {},
   "source": [
    "Above we've calculated the cosine similarity > 0.99 for the similar images. Now replace one image with another and run the cells again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4afc9e5",
   "metadata": {},
   "source": [
    "## Find similar images with cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50035e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "\n",
    "#check your device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "#select the model for the embeddings. Here we use a Vision Transformer (ViT)\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "#create a list of the given images\n",
    "dataset_folder = './elephants/'\n",
    "images = []\n",
    "for root, dirs, files in os.walk(dataset_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(('jpg','jpeg')):\n",
    "            images.append(  root  + '/'+ file)\n",
    "\n",
    "\n",
    "#Set the input image and create it's embedding\n",
    "original_image = './elephants/1200px-Elephant_near_ndutu.jpg'\n",
    "input_image = preprocess(Image.open(original_image)).unsqueeze(0).to(device) #\n",
    "input_image_features = model.encode_image(input_image)\n",
    "\n",
    "# Create embeddings for other images, do similarity search and store in a python dictionary.\n",
    "result = {}\n",
    "for img in images:\n",
    "    with torch.no_grad():\n",
    "        image_preprocess = preprocess(Image.open(img)).unsqueeze(0).to(device)\n",
    "        image_features = model.encode_image(image_preprocess)\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "        sim = cos(image_features[0],input_image_features[0]).item()\n",
    "        sim = (sim+1)/2\n",
    "        result[img]=sim\n",
    "\n",
    "\n",
    "#sort the results and print the top 3\n",
    "sorted_value = sorted(result.items(), key=lambda x:x[1], reverse=True)\n",
    "sorted_res = dict(sorted_value)\n",
    "top_3 = dict(itertools.islice(sorted_res.items(), 3))\n",
    "print(top_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832022d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display most similar images\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "original_image = original_image\n",
    "first = list(top_3.keys())[0]\n",
    "second = list(top_3.keys())[1]\n",
    "third = list(top_3.keys())[2]\n",
    "\n",
    "#original image\n",
    "img0 = Image(original_image, width = 400) \n",
    "\n",
    "#top 3\n",
    "img1 = Image(first, width = 400) \n",
    "img2 = Image(second, width = 400) \n",
    "img3 = Image(third, width = 400)\n",
    "\n",
    "print(\"The original image is:\")\n",
    "display(img0)\n",
    "print('The duplicate is: ')\n",
    "display(img1)\n",
    "print('And the most similar images are:')\n",
    "display(img2, img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f5a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "802e727b",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "Now do this on your own dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c4de2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
